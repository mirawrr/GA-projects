{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6300e8",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Subreddit Classifier - Tea or Coffee?\n",
    "By Amira (DSI-28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb861e",
   "metadata": {},
   "source": [
    "---\n",
    "# Problem Statement\n",
    "\n",
    "We are a team of data scientists working for Coffea Vibes, a beverage company. The company is venturing into e-commerce and will be launching its own website/application selling coffee and tea products to consumers. We have been tasked to build a classification model that can accurately distinguish between coffee and tea in textual data.\n",
    "\n",
    "Our classification model will contribute to the following use cases:\n",
    "\n",
    "1. Our web development team can optimise the recommender systems so as to accurately suggest related products and advertisements to our potential consumers who might have varying preferences for coffee or tea.\n",
    "2. Our business insights team can leverage on the classification model to correctly distinguish customer feedback on coffee and tea (e.g. through emails) and comments made on the company's social media pages, to aid better understanding of customer's feedback and take appropriate actions quickly, if necessary.\n",
    "\n",
    "We evaluated the models based on the following criteria:\n",
    "\n",
    "1. Accuracy scores (the higher, the better)\n",
    "2. Delta between train and test scores (the smaller, the better)\n",
    "3. Clear distinction of important features i.e. words to distinguish coffee and tea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4386d",
   "metadata": {},
   "source": [
    "---\n",
    "# Structure\n",
    "\n",
    "To organise my work better, I have organised this project into three notebooks: \n",
    "\n",
    "* Notebook 1 : Data Acquisition\n",
    "* Notebook 2: Data Cleaning & Exploratory Data Analysis\n",
    "* Notebook 3: Modelling & Model Evaluation\n",
    "\n",
    "<span style='color:red'>**This is Notebook 1.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac43e9",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26983c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries/packages\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7c1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined a function that requires input of subreddit name. \n",
    "# this function will perform a for loop for x times to scrape reddit submission posts (100 posts each time).\n",
    "\n",
    "def my_webscraper(subreddit,loops=20):\n",
    "\n",
    "    data = pd.DataFrame() # create an empty dataframe which will store all the scraped data\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "    for i in range(loops): # range is 10 bc i want to repeat this 10 times (each time pull only 100 posts), can change to other numbers. \n",
    "        if i == 0: # for the first time, 'before' param  will take the most recent value as of when this code is run. \n",
    "            params = {'subreddit' :str(subreddit),'size' : 100}\n",
    "        else: # for the subsequent runs,  change 'before' to the 'created_utc' value of the prev scraped set. \n",
    "            params = {'subreddit' :str(subreddit),'size' : 100,'before' : bef}\n",
    "\n",
    "        # the set of codes below will repeat for each iteration of i. \n",
    "        res = requests.get(url,params)\n",
    "        results = res.json()\n",
    "        posts = pd.DataFrame(results['data'])\n",
    "        bef = posts.iloc[-1]['created_utc']\n",
    "        data = data.append(posts,ignore_index=True)\n",
    "        print(f'This is iteration no. {i+1}, status code is {res.status_code}, accumulated no. of rows extracted is {len(data)}.')\n",
    "        time.sleep(5) # set a timer between each iteration of i.\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22a7ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# increased the no. of loops for 'prolife' subreddit as it contains more removed/deleted posts\n",
    "\n",
    "coffee = my_webscraper(subreddit='coffee')\n",
    "print(coffee.shape)\n",
    "print(coffee.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tea = my_webscraper(subreddit='tea')\n",
    "print(tea.shape)\n",
    "print(tea.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b256fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dfs to csv to work with in notebook2\n",
    "# files have been saved in the data folder; commenting this out in case we overwrite the data which is extracted live...\n",
    "\n",
    "# coffee.to_csv('./data/coffee.csv',index=False,na_rep=' ')\n",
    "# tea.to_csv('./data/tea.csv',index=False,na_rep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
